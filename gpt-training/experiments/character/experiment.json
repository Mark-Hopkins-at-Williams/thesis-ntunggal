{
    "tokenizer": {
        "name": "CharacterTokenizer",
        "max_vocab_size": 50257,
        "max_examples": "",
        "special_tokens": {
            "unk_token": "<unk>",
            "bos_token": "<bos>",
            "eos_token": "<eos>",
            "pad_token": "<pad>"
        },
        "tokenizer_files_dir": "/mnt/storage/ntunggal/tokenizer-files/character",
        "vocab_file_name": "vocab.json",
        "input_method": ""
    },
    "model": {
        "n_positions": 1024,
        "n_ctx": 1024,
        "n_embd": 768,
        "n_layer": 6,
        "n_head": 12,
        "activation_function": "gelu"
    },
    "training": {
        "eval_steps": 1000,
        "max_steps": 20000,
        "batch_size": 16,
        "learning_rate": 5e-5,
        "weight_decay": 0.01,
        "warmup_steps": 500,
        "max_grad_norm": 1
    }
}
